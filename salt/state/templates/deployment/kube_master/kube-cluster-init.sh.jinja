#!/bin/bash

function kubectl_apply()
{
    local uri="${1}"
    curl -s "${uri}" | sed -i '/imagePullPolicy/ s/Always/IfNotPresent/' | kubectl apply -f -
}

function untaint_master()
{
    kubectl taint nodes --all node-role.kubernetes.io/master-
}

function create_helm_svcaccount()
{
    kubectl create -f - <<-EOF
		apiVersion: v1
		kind: ServiceAccount
		metadata:
		  name: tiller
		  namespace: kube-system
		---
		apiVersion: rbac.authorization.k8s.io/v1beta1
		kind: ClusterRoleBinding
		metadata:
		  name: tiller
		roleRef:
		  apiGroup: rbac.authorization.k8s.io
		  kind: ClusterRole
		  name: cluster-admin
		subjects:
		  - kind: ServiceAccount
		    name: tiller
		    namespace: kube-system
	EOF
}

function create_metallb_configmap()
{
    local tmpf=$(mktemp -t tmp_metallb_cfgmap_XXXXXX.yaml)

    cat > "${tmpf}" <<-EOF 
			apiVersion: v1
			kind: ConfigMap
			metadata:
			    namespace: metallb-system
			    name: config
			data:
			    config: |
			        {{config.metallb.config.config|indent(8)}}
		EOF

    cat "${tmpf}"
    kubectl create -f "${tmpf}"
}

function configure_helm()
{
    {%- if 'helm' in config and config.helm and 'enabled' in config.helm and config.helm.enabled %}
    {%-     set helm = config.helm %}

    # add on cluster admin privs to the default service account
    # so that the tiller started by helm can do its work.
    # kubectl create clusterrolebinding add-on-cluster-admin --clusterrole=cluster-admin --serviceaccount=kube-system:default

    create_helm_svcaccount

    local -a init_flags=(
        --service-account tiller \
        {%- if 'charts' in helm and helm.charts %}
        --stable-repo-url {{helm.charts}} \
        {%- endif %}
        --tiller-image    {{helm.registry}}/kubernetes-helm/tiller:{{helm.version}} 
    )

    helm init "${init_flags[@]}"

    {%- if 'charts' in helm and helm.charts %}
    helm repo add nexusrepo "{{helm.charts}}"
    helm repo update
    {%- endif %}

    {%- if 'metallb' in config and config.metallb and 'enabled' in config.metallb and config.metallb.enabled %}

    #helm install  --name metallb stable/metallb 

    # This works but displays the following warnings
    #MetalLB is now running in the cluster.
    #WARNING: you specified a ConfigMap that isn't managed by
    #Helm. LoadBalancer services will not function until you add that
    #ConfigMap to your cluster yourself.


    #helm install  --name metallb \
    #    --set  arpAddresses=192.168.16.240/30 \
    #    --set  controller.image.repository=nexus:7082/metallb/controller \
    #    --set  speaker.image.repository=nexus:7082/metallb/speaker \
    #    stable/metallb 
    
    kubectl create namespace metallb-system
    create_metallb_configmap

    helm install  --name metallb \
        --set  controller.image.repository=nexus:7082/metallb/controller \
        --set  speaker.image.repository=nexus:7082/metallb/speaker \
        stable/metallb 
    
    {%- endif %}

    {%- endif %}
}

function configure_dashboard()
{
    {%- if 'dashboard' in config and config.dashboard and 'enabled' in config.dashboard and config.dashboard.enabled %}
    {%-     set dashboard = config.dashboard %}
    {%-     if 'config' in dashboard %}
    kubectl_apply '{{dashboard.config}}'
    {%-     endif %}
    {%- endif %}
}

function configure_cni()
{
    {%- if 'cni' in config and config.cni %}
    {%-     set cni = config.cni %}
    {%-     if 'config' in cni %}
    kubectl_apply '{{cni.config}}'
    {%-         if cni.config|regex_match('(kuberouter)') %}
    kubectl -n kube-system delete ds kube-proxy
    docker run --privileged --net=host k8s.gcr.io/kube-proxy-amd64:{{config.version}} kube-proxy --cleanup
    {%-         endif %}
    {%-     endif %}
    {%- endif %}
}

function doit()
{
    echo "Init kube cluster at date $(date)"
    kubeadm reset

    systemctl restart kubelet

    kinit_flags=(
        --kubernetes-version {{config.version}}

        {%- if 'api_advertise_address' in config and config.api_advertise_address %}
        --apiserver-advertise-address '{{config.api_advertise_address}}'
        {%- endif %}

        {%- if 'pod_network_cidr' in config and config.pod_network_cidr %}
        --pod-network-cidr '{{config.pod_network_cidr}}'
        {%- endif %}

        --node-name '{{config.master_name}}'
    )

    kubeadm  init "${kinit_flags[@]}"

    export KUBECONFIG=/etc/kubernetes/admin.conf 

    {%- if 'single_node' in config and config.single_node %}
    untaint_master
    {%- endif %}

    configure_cni

    configure_dashboard

    configure_helm
}

doit > /var/log/kube-cluster-init.log 2>&1
