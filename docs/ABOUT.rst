.. _about_soestack:

##############
About SoeStack
##############

SoeStack is a SOE designed to simplify the deployment of a Linux environment. 

It started as a 'playground' for me to try out different things with SaltStack, such as:

    - different ways of layering configuration data

    - writing custom SaltStack modules

    - possibilities for integrating SaltStack with IPA

It does not intend to be a SOE that does everything, but simply a base from which a SOE can quickly be built. To that end it supports initial building of an infrastructure server, followed by deployment of other node types by various methods (PXE, vagrant) with a demo configuration which can be customised or used as a basis for a new SOE, which can then further be extended.

It may not be entirely in a working state, I only work on the parts I'm interested in, in my free time and I may have break the older parts at any time. For example I've recently refactored the vagrant provisioning but may have broken the USB provisioning. Good luck, feel free to file a bug report or request help.

In its current state, it is primarily designed to support:

    - CentOS, RedHat, or Fedora based distros (tested only with CentOS, however should be quite compatible with RedHat and Fedora)

    - Deploying various services such as:

        - An IPA server for user / identity management

        - Gitlab (Community Edition)

        - A monitoring service and dashboard (Prometheus and Grafana)

        - An ElasticSearch stack (ElasticSearch, LogStash, and Kibana)

        - A small kubernetes cluster

        - A pull-through package repository for caching software from the internet

    - Configuring nodes, users, software selections etc within a SaltStack configuration area

    - Building nodes via USB, network (PXE) or virtual machines (via Vagrant).

    - Clamav antivirus

    - Schema checking for important configuration data

Issues
######

  - Initial infra server deployment from a USB can need the salt state applied an extra time or two before everything is working

  - NetworkManager is forever a problem. It seems like whatever you do, they find a way to break it.

    + ``Generated by NetworkManager`` anyone? ``NM_CONTROLLED=no`` on every interface. Disabled the service? They will still find a way to ruin your day. Again, and again, and again two weeks later when you didn't change a thing.

Philosophies
############

You may notice that the SOE does not do everything in the best way..

Nothing's perfect ;)

This SOE started as a place to experiment with various ideas, such as those that follow, and some of the experiments could be improved:

    - Can we declare configuration data in an object-oriented kind of way

    - Can we apply states in an object-oriented kid of way

    - What techniques can we utilise to take advantage of re-use

    - Can we provide a library of routines or functionalit suitable for re-use, and use it to 'pull in' components or 'compose' an overall design.

The architecture of SaltStack has made this more troublesome than expected, for example:

    - The master->minion communication channel is primarily through the pillar only - in the sense that the master, which has 'all the information' builds the pillar data for the minions, but after that, it is the minion itself which decides which states to run, by processing the state files.

    - This means that at the end of the day, any data to encode a flow of control for generating/selecting particular states must be:

        + explicitly declared in the state files (essentially hard coded, a hierarchy of includes, perhaps utilising ``grains``), or

        + have flow-of-control information included within the salt pillar data
        
            * this has the unfortunate side effect that the state files become programs in-and-of-themselves, they must utilise something like ``jinja`` to process the pillar data and decide which states to emit.

        + declared explicitly via various pattern matching, in the ``top file``

This SOE represents various techniques I've experimented with in order to achieve a more ``declarative``, and ``automatic`` mode of operations.

For example:

    - The ``nugget`` concept:

        + nuggets were an idea of a way that inheritance / overriding could be implemented - in such a way that various data could be simply declared, and inherited or reused by reference, in different ``nugget`` classes.

            * I would classify this as a failed experiment, which adds more complexity than it reduces.

                ^ It does have benefits, for example every ``deployment`` in this project is treated as an instance of a similarly named ``nugget`` class, 
                  and things like package installation, directory creation, firewall rules, templated file generation with data from the deployment ``config`` can 
                  largely be performed simply by declaring the data ( package sets, firewall rulesets, named templates, files, and config data), and then 
                  with an appropriately named nugget. Deployments each have a deployment type which is mapped to a nugget class and this stuff at least is 
                  performed automatically via a jinja include statement. 

                ^ The downside is that despite conventions such as nugget names matching deployment types, this:

                    > from experience is not evident to a newcomer or user of the software without significant explanation or documentation

                    > can easily be broken (the nugget data was ``declared`` but was never ``included``) with extremely little troubleshooting support from salt

    - The ``secrets`` concept:

        + One of the things that strikes you when you are ``rolling your own`` SOE, is how many tools require a password on the commandline, or else 
          entered manually. 
        
        + It is a common thing to find administrative scripts with hard coded passwords, even if the author set the password to ``change me`` or something else to indicate that
          the example password should be changed before deployment, they often aren't, and they would still be in plain text anwway.

        + I wanted to apply a kind of concept similar to that found in ``kubernetes`` for having ``secrets``, that are at least encrypted in all places on the filesystem,
          such that at least there is a chain of decryption keys which can be protected with greater though. 

            * This is just starting to be implemented in this SOE. Once the salt master is available, it can:
            
                ^ generate random secrets, and

                ^ distribute them to (selected) minions, and

                ^ the minions can utilise and decrypt those secrets using their own key, and 

                ^ the minions can then run whichever tool they require, using those temporarily-decrypted secrets.

            * For the moment, this is using OpenSSL encryption, which has a length limitation on the size of the secrets which can be saved.

            * Some work has been done to provide GPG encryption, for arbitrary length secrets, with a chain of trust.

        + Eventually most secrets will be encrypted in one form or another, from the initial installation of a server (for example a physical server built 
          from a USB stick with a build USB and an extra USB with encryption keys on it).

        + Eventually, the aim is these keys can be utilised to configure disk encryption at installation time, possibly with a network service to 
          provide the encryption keys to trusted hosts when they boot, so they are protected at rest (``when turned off``). 
          Things I'm just starting to look at here:

            * TPM support, PXE support, UEFI support, and the combination of the all these, especially with new hardware more and more utilising UEFI and having limited legacy boot support.

        + What's currently implemented:

            * Once the salt master is deployed, it can generate and store encrypted tokens or passwords 

            * It can distribute a token over an encrypted channel to the minion 

            * The minion can save it to an encrypted storage for later use 

            * All scripts can utilise a helper script/routine to temporarily gain access to the
              decrypted secret value (if it has been received from the master) instead of utilising configuration
              files with plain text values

        + Secrets are sent to minions via pillar data (encrypted over salt communication channels) and can 
          be stored to an (encrypted) file local to the minion, where scripts locally executing as the root user
          can decrypt the secret, utilising the secret local to the minion.  The data sent is encrypted with a 
          key specific to each minion.
    
    - IPA integration:

        + It seems that no-one had done IPA integration with salt before, that I could find anyway.

            * The ``saltip`` module provided is less than ideal and simply combines the concepts of:

                ^ generating a ticket for salt to use (an administrator must renew it within a set period, could be a problem when administrators go on holiday)

                ^ applying some state defined in salt pillar data, so that the salt pillar data (easily configuration-managed) is the source of truth, rather
                  than relying on just whatever IPA state has been built over time after successive administrative modifications

            * Ideally this module would be re-written to utilise the python ``ipa`` modules directly
        
        + At the very least, this module provides the ability to:
        
            * declare and manage defined hosts and DNS entries within CM'd salt pillar data


    - Schemas:

        + The provided schema module provides a way to declare data types used within the pillar data and
          to validate that and provide warnings or errors within the pillar. 

            * Schemes are applied as a salt extension module which processes the pillar data at a late stage,
              after other modules have done their work

    - Postprocessing:

        + With salt, the primary data format is yaml, but with various extensions for hierarchical loading of files.

        + The ``postproc`` module provids a simpler, more fault-tolerant way of re-using data that you have defined wihtin your salt pillar.

            * The postproc module can be stacked late in the sequence of pillar extension modules, in order to perform replacements of 
              data declared earlier, where a convenient shortcut is used. 

    - Troubleshooting:

        + With salt, when troubleshooting, it is always useful to know which files were loaded, when, in what order. Salt doesn't provide
          this out of the box, which can be troublesome including files based in a hierarchy of layers based on dynamic values. The ``loadtracker`` 
          module provides a way of tracking each pillar file that is loaded, in the sequence that it is loaded, and adding this information to
          the salt pillar data for inspection by a developer or administrator.

    - NO-OP notices:

        + Salt does not provide out of the box any way for the state hierarchy to produce a state which simply notifies the 
          administrator of a situation, or that a state was or was not run for a particular reason. The ``noop`` module included,
          provides an easy way for SLS files to emit states that provide information without necessarily reporting a change in 
          state, or, explicitly reporting an error status with more information, while still having no side effect other than 
          the reporting of the problem.

            * This module provides ``notice``, ``error``, ``warning``, ``pprint``, and ``json`` states.

    - UUIDs:

        + Once you start using jinja macros and includes, it can be troublesome to produce a whole set of states
          with unique names (in the yaml format utilised by salt, each state produced must have a unique name). This 
          module provides state name suffixes or prefixes based of a uuid to reduce the chances of a conflicting 
          state name.

    - The current time:

        + Sometimes, you just really want to know what is the current time. This module provides the current time to the caller
          (requires jinja to access it).


.. _about_getting_started:

Getting Started
###############

To just try it out and see what it does, please see :ref:`quickstart`.

Before you can use the files for your own purposes you will need to:

    - Determine your desired network configuration

    - Generate default root and grub passwords (see :ref:`modifying default passwords`)

    - Choose a name for your SOE, and a name for your local network (LAN)

    - Copy the 'demo' layer(s) within the pillar area and rename to use your new SOE name or LAN name

    - in various places, you will then need to change ``demo`` to your new SOE name or layer name, for example:

        + the variable ``ss.LAYERS``, which is by default set to ``soe:demo,site:testing,lan:example``

    - for example to use a SOE name ``example``, a site name ``testsite``, a lan name ``testlan``:
    
        + copy ``salt/pillar/soe/demo`` to ``salt/pillar/soe/example``

        + copy ``salt/pillar/layers/soe/demo.sls`` to ``salt/pillar/layers/soe/example.sls``

        + copy ``salt/pillar/layers/site/testing.sls`` to ``salt/pillar/layers/site/testsite.sls``

        + copy ``salt/pillar/layers/lan/example.sls`` to ``salt/pillar/layers/lan/testlan.sls``

        + copy ``salt/pillar/layers/lan/example/*`` to ``salt/pillar/layers/lan/testlan/*``

        + change ``ss.LAYERS`` to ``soe:example,site:testsite,lan:testlan``

    - In general, the layers defined with ss.LAYERS are used to select the salt configuration files used for any node

    - Customise various network settings, domain names, IP addresses, the GATEWAY, etc

    - Download various RPM packages required to 'bootstrap' an environment. The binary files required are not included in this SOE.

    - Either configure a Sonatype Nexus instance with various repositories to access files from the internet, and cache them, or else modify the nexus repos predefined within the SOE.

        + If you choose to use the predefined configuration, you will need to create 'registries' within your Nexus instance to match the configuration found within the salt pillar key ``nexus.repos`` and ``nexus.blobstores``.

        + Alternatively modify the pillar data to match your existing nexus repos

.. _about_deploying_with_usb:

Deploying an infrastructure server via USB
##########################################

Once you've modified all the configuration variables, you can generate a USB thumb drive that can be used to install a new server.

You will need to determine the Model name of the USB device and update various files within the ``provision/usb`` subfolder.

Make sure to specify the correct block device name so that you do not overwrite the wrong device.

The included ``ss-bundler`` tools require ``python3`` and the ``guestfs`` python modules, with the system configured with libvirtd support installed and enabled. The ``guestfs`` software is utilised for formatting a disk image or disk device and adding files to it.

Binary files will need to be added to the ``bundled`` subfolder, such as:

    - docker image files (primarily Sonatype Nexus OSS version)
    
    - various RPM files from different online repositories (centos, rpmfusion, epel, docker community edition)

    - (optional, if you will run nexus on this machine):

        + a Nexus ``blobs`` tarball and ``db-backup.tar``

        + if present, these will be used to bootstrap a new Nexus instance on the machine

The USB provisioning, if the ss.STANDALONE=1 flag is set, will set the server up as an infrastructure server which provides:

    - an IPA server

    - monitoring and a dashboard

    - a SaltStack master for controlling other nodes

    - a PXEboot service configured for building other nodes from this server, using SoeStack

    - a Kubernetes master deployment, to which other nodes can be added later

    - an ElasticSearch stack, for logging and log inspection

    - The ability to define package sets with lists of different packages to be installed on different node types.

    - IPA integration for SaltStack which is able to maintain DNS addresses configured within the SaltStack ``pillar`` data.

    - A print server (CUPS)

    - Various development tools:

        + python / pip

        + nodesource npm

    - A configured email service

    - Bash / profile settings

Where it may go
###############

Areas I'm interested in extending/improving it:

    - more work to automate provisioning and configuration of all services using SSL/https support with proper certificates from the IPA certificate authority

        + this isn't hard, as the IPA services provide a certificate authority which can generate certificates, the work just hasn't been done to automate this

        + would really like to do this and have all SOE services preconfigured to be deployed in a secure setup by default

    - I have heard anecdotally from organisations using it that the USB provisioning (using python guestfs support) is not usable within centos

        + this means even though the SOE is primarily for CentOS/RedHat environments, the USB provisioning currently requires a Fedora or other more modern setup to run successfully

        + it is possible that a CentOS/RedHat 8 install would provide new-enough libguestfs and python guestfs module support

    - (optionally) deploy the SOE services within a specified kubernetes cluster

        + this would be optional as it would require the administrator to have an understanding of kubernetes, and kubernetes-specific maintenance such as period renewal of cluster service account certs so that services do not stop working after a year.

        + it looks like it may actually be possible to deploy even the IPA server within a kubernetes cluster

        + if this can be done then providing the SOE services can be simply a matter of provisioning one infrastructure server running the kubernetes cluster and then adding resources to it as required / as they are available.

    - more testing with fedora clients (currently it's only tested with CentOS) to allow a more 'current' and less 'enterprise' environment, where that's suitable

    - more 'SOE' support work, in terms of having things autoconfigured or locked down according to SOE settings such as:

        - automatic email service autoconfiguration (partly done)

        - automatic browser settings (partly done)

        - enforced/locked-down screen saver settings (possible with gnome mandatory settings and KDE kiosk settings at least)

        - preconfigured desktop icons and browser bookmarks for your work environment

    - convert salt/IPA integration module to use IPA python modules directly instead of IPA commandline interface

    - looking at adding deployments for BitBucket, JIRA, and Confluence

        + support free versions for developers/testing but also support paid versions for organisations that have purchased licenses
    
    - Support CentOS/RedHat 8

        + CentOS / RedHat 8 introduces various issues such as:
        
            * making NetworkManager pretty much mandatory (no legacy network init script support at all)

            * python 2 / python 3 naming schemes have changed in a non-backwards-compatible way

            * primarily this affects network configuration in environments where NetworkManager causes issues, such as with docker and kubernetes, and ensuring a smooth installation of SaltStack without broken package dependencies

    - support SaltStack running on Python 3

        + currently the ``jinja`` templating within the canned SOE demo files requires the Python2 salt implementation

            * this should be a simple matter of changing all ``iteritems`` uses to ``items``

            * package selection is problemetic with RedHat / CentOS 8 due to how they've chosen to (re)name their python 3 and python 2 packages. 

                # saltstack packages for Python 3 currently require RPM package names that use the CentOS/RedHat 7 naming conventions

    - would like to provide a mapping between selected configurations and Australian Government ISM (Information Security Manual) controls.

    - using Nexus as a software repository mirror has some fair drawbacks, I would like to try to integrate Artifactory for an example as an alternative for organisations that can pay for a license.

